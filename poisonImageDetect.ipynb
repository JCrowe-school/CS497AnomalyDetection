{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2639785",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb62e42-d8bc-4132-bf0c-58c3d2c8c02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15.2a0\n"
     ]
    }
   ],
   "source": [
    "# due to warning from above, below steps imported torchvision\n",
    "#print(torch.__version__)\n",
    "#!conda install -c pytorch torchvision=0.14.1 -y\n",
    "#import torchvision\n",
    "#print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc55ce8b-07a1-4185-ab2c-fe00b14d7905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b546c721-4d3e-4279-99a4-4f4140624b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3/envs/mypy3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#load pretrained model\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "#modify output layer for 2 classes - poisoned vs non-poisoned images\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_save/model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e1568c85-cbf4-4af2-83a4-18d516f051fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze earlier layers - this is optional and it may work better to NOT freeze the earlier layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"fc\" in name:\n",
    "            param.requires_grad = True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d142d7b-b135-48d0-a17a-a76e99a31c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# comment out this or cifar as needed\n",
    "#trainset = torchvision.datasets.ImageFolder(root=\"data/split/train\",transform=transform)\n",
    "#trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "#testset = torchvision.datasets.ImageFolder(root=\"data/split/train\",transform=transform)\n",
    "#testloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "cifar10 = torchvision.datasets.CIFAR10(root='./data/clean', train=True, download=False, transform=transform)\n",
    "\n",
    "# relabel cifar10 to make frogs 0 and trucks 1\n",
    "new_labels = []\n",
    "for label in cifar10.targets:\n",
    "    if label == 6:\n",
    "        new_labels.append(0)\n",
    "    elif label == 9:\n",
    "        new_labels.append(1)\n",
    "    else:\n",
    "        new_labels.append(-1) #everything else becomes -1\n",
    "cifar10.targets = new_labels\n",
    "\n",
    "# create the dataset to then make the dataloader\n",
    "frog_indices = [i for i, label in enumerate(cifar10.targets) if label == 0]  \n",
    "truck_indices = [i for i, label in enumerate(cifar10.targets) if label == 1]  \n",
    "frog_truck_indices = frog_indices + truck_indices\n",
    "frog_truck_dataset = torch.utils.data.Subset(cifar10, frog_truck_indices)\n",
    "trainloader = torch.utils.data.DataLoader(frog_truck_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='./data/clean', train=False, download=False, transform=transform)\n",
    "\n",
    "test_labels = []\n",
    "for label in cifar10_test.targets:\n",
    "    if label == 6:\n",
    "        test_labels.append(0)\n",
    "    elif label == 9:\n",
    "        test_labels.append(1)\n",
    "    else:\n",
    "        test_labels.append(-1)\n",
    "cifar10_test.targets = test_labels\n",
    "\n",
    "frog_test_indices = [i for i, label in enumerate(cifar10_test.targets) if label == 0]\n",
    "truck_test_indices = [i for i, label in enumerate(cifar10_test.targets) if label == 1]\n",
    "frog_truck_test_indices = frog_test_indices + truck_test_indices\n",
    "frog_truck_test_dataset = torch.utils.data.Subset(cifar10_test, frog_truck_test_indices)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(frog_truck_test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "EPOCHS=40\n",
    "BATCH_SIZE = 32\n",
    "TOTAL_SIZE = 9800\n",
    "#TOTAL_TEST_SIZE = \n",
    "STEPS_PER_EPOCH = TOTAL_SIZE // BATCH_SIZE\n",
    "\n",
    "TOTAL_TEST_SIZE = 200 \n",
    "STEPS_PER_TEST_EPOCH = TOTAL_TEST_SIZE // BATCH_SIZE\n",
    "#testset = torchvision.datasets.CIFAR10(root='./data/clean', train=False, download=False, transform=transform)\n",
    "#testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be43e011-fdf5-4d02-9cb7-247c0884e508",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainset\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainset' is not defined"
     ]
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73e855ae-330b-48df-83b9-f1ebd8c9fe88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frog', 'truck']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63564e38-9eba-4231-8a85-637f10ed2973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frog count: 5000, Truck count: 5000\n"
     ]
    }
   ],
   "source": [
    "# note, above two really only work if not using cifar\n",
    "frog_count = sum(cifar10.targets[i] == 0 for i in frog_truck_indices)\n",
    "truck_count = sum(cifar10.targets[i] == 1 for i in frog_truck_indices)\n",
    "\n",
    "print(f\"Frog count: {frog_count}, Truck count: {truck_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f096393e-2766-4dad-9f34-b31d5bba5c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "796909e5-930b-4a8f-9cb5-e1871beb3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Sending model to device\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=0.0007\n",
    ")  # lr should be kept low so that the pre-trained weights don't change easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1254f433-6658-43cc-a2ff-ead0d19cf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test():\n",
    "    test_loss = []\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(testloader):\n",
    "        if batch_idx == STEPS_PER_TEST_EPOCH:\n",
    "            break\n",
    "\n",
    "        # Model is used to predict the test data so we are switching off the gradient\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.long().to(device)\n",
    "            output = model(data)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Note that optimizer is not used because the model shouldn't learn the test dataset\n",
    "\n",
    "            for i in range(BATCH_SIZE):\n",
    "                a = []\n",
    "                for j in output[i]:\n",
    "                    a.append(float(j.detach()))\n",
    "\n",
    "                pred = a.index(max(a))\n",
    "\n",
    "                if pred == int(target[i]):\n",
    "                    correct = correct + 1\n",
    "\n",
    "                else:\n",
    "                    incorrect = incorrect + 1\n",
    "\n",
    "        test_loss.append(float(loss.detach()))\n",
    "    print(\"CORRECT: \" + str(correct), \"INCORRECT: \" + str(incorrect),\"TEST ACCURACY: \"+str(correct/(correct+incorrect)))\n",
    "    return (\n",
    "            correct/(incorrect+correct),\n",
    "            sum(test_loss)/len(test_loss),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "53b65a87-112f-4d94-914c-c67893873244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "ACCURACY AND LOSS BEFORE TUNING\n",
      "ACCURACY : 1.0 LOSS : 0.00025489663797391887\n"
     ]
    }
   ],
   "source": [
    "acc_ , loss_ = get_test()\n",
    "print(\"ACCURACY AND LOSS BEFORE TUNING\")\n",
    "print(\"ACCURACY : \"+str(acc_),\"LOSS : \"+str(loss_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84fea5f4-2a53-4cee-ad97-f569f5d5de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_save/model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ba9ad283-a8be-4f84-add2-56b6e63fc018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------EPOCH 0 -----------------------------------\n",
      " EPOCH 0 MINIBATCH: 305/306 LOSS: tensor(3.0286e-06, device='cuda:0') \n",
      " EPOCH 0 LOSS tensor(0.0005, device='cuda:0') ETA: 2.416653633117676 \n",
      " MAX LOSS: tensor(0.0242, device='cuda:0') MIN LOSS: tensor(2.6077e-08, device='cuda:0') TRAIN ACCURACY: 0.9997957516339869\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 1 -----------------------------------\n",
      " EPOCH 1 MINIBATCH: 305/306 LOSS: tensor(3.9784e-06, device='cuda:0') \n",
      " EPOCH 1 LOSS tensor(0.0002, device='cuda:0') ETA: 2.4780585765838623 \n",
      " MAX LOSS: tensor(0.0226, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 2 -----------------------------------\n",
      " EPOCH 2 MINIBATCH: 305/306 LOSS: tensor(0.0002, device='cuda:0') 0')\n",
      " EPOCH 2 LOSS tensor(0.0012, device='cuda:0') ETA: 2.333292007446289 \n",
      " MAX LOSS: tensor(0.1476, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9996936274509803\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 3 -----------------------------------\n",
      " EPOCH 3 MINIBATCH: 305/306 LOSS: tensor(1.0803e-07, device='cuda:0') \n",
      " EPOCH 3 LOSS tensor(0.0003, device='cuda:0') ETA: 2.238844871520996 \n",
      " MAX LOSS: tensor(0.0349, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 4 -----------------------------------\n",
      " EPOCH 4 MINIBATCH: 305/306 LOSS: tensor(3.7253e-09, device='cuda:0') \n",
      " EPOCH 4 LOSS tensor(0.0003, device='cuda:0') ETA: 2.2530694007873535 \n",
      " MAX LOSS: tensor(0.0172, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 1.0\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 5 -----------------------------------\n",
      " EPOCH 5 MINIBATCH: 305/306 LOSS: tensor(7.0780e-08, device='cuda:0') \n",
      " EPOCH 5 LOSS tensor(0.0002, device='cuda:0') ETA: 2.49078631401062 \n",
      " MAX LOSS: tensor(0.0179, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 1.0\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 6 -----------------------------------\n",
      " EPOCH 6 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0')cuda:0') \n",
      " EPOCH 6 LOSS tensor(0.0001, device='cuda:0') ETA: 2.3313467502593994 \n",
      " MAX LOSS: tensor(0.0049, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 1.0\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 7 -----------------------------------\n",
      " EPOCH 7 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0') 0'):0')\n",
      " EPOCH 7 LOSS tensor(0.0003, device='cuda:0') ETA: 2.168074607849121 \n",
      " MAX LOSS: tensor(0.0362, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9997957516339869\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 8 -----------------------------------\n",
      " EPOCH 8 MINIBATCH: 305/306 LOSS: tensor(0.0994, device='cuda:0') 0')\n",
      " EPOCH 8 LOSS tensor(0.0006, device='cuda:0') ETA: 2.409493923187256 \n",
      " MAX LOSS: tensor(0.0994, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9996936274509803\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 9 -----------------------------------\n",
      " EPOCH 9 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0') uda:0')\n",
      " EPOCH 9 LOSS tensor(0.0003, device='cuda:0') ETA: 2.159623622894287 \n",
      " MAX LOSS: tensor(0.0325, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9997957516339869\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 10 -----------------------------------\n",
      " EPOCH 10 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0') uda:0')\n",
      " EPOCH 10 LOSS tensor(0.0005, device='cuda:0') ETA: 2.2612557411193848 \n",
      " MAX LOSS: tensor(0.0613, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9996936274509803\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 11 -----------------------------------\n",
      " EPOCH 11 MINIBATCH: 305/306 LOSS: tensor(3.7253e-09, device='cuda:0') \n",
      " EPOCH 11 LOSS tensor(0.0002, device='cuda:0') ETA: 2.2688186168670654 \n",
      " MAX LOSS: tensor(0.0167, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 1.0\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 12 -----------------------------------\n",
      " EPOCH 12 MINIBATCH: 305/306 LOSS: tensor(1.2293e-07, device='cuda:0') \n",
      " EPOCH 12 LOSS tensor(0.0004, device='cuda:0') ETA: 2.407170295715332 \n",
      " MAX LOSS: tensor(0.0954, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 13 -----------------------------------\n",
      " EPOCH 13 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0') uda:0')\n",
      " EPOCH 13 LOSS tensor(0.0001, device='cuda:0') ETA: 2.654742479324341 \n",
      " MAX LOSS: tensor(0.0187, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 1.0\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 14 -----------------------------------\n",
      " EPOCH 14 MINIBATCH: 305/306 LOSS: tensor(1.9744e-07, device='cuda:0') \n",
      " EPOCH 14 LOSS tensor(0.0006, device='cuda:0') ETA: 2.5037789344787598 \n",
      " MAX LOSS: tensor(0.1250, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9997957516339869\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 15 -----------------------------------\n",
      " EPOCH 15 MINIBATCH: 305/306 LOSS: tensor(3.7253e-09, device='cuda:0') \n",
      " EPOCH 15 LOSS tensor(0.0008, device='cuda:0') ETA: 2.575092077255249 \n",
      " MAX LOSS: tensor(0.1114, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9997957516339869\n",
      "CORRECT: 188 INCORRECT: 4 TEST ACCURACY: 0.9791666666666666\n",
      "-----------------------EPOCH 16 -----------------------------------\n",
      " EPOCH 16 MINIBATCH: 305/306 LOSS: tensor(2.1770e-05, device='cuda:0') \n",
      " EPOCH 16 LOSS tensor(0.0010, device='cuda:0') ETA: 2.5278663635253906 \n",
      " MAX LOSS: tensor(0.1212, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9996936274509803\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 17 -----------------------------------\n",
      " EPOCH 17 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0')cuda:0') \n",
      " EPOCH 17 LOSS tensor(0.0014, device='cuda:0') ETA: 2.3095929622650146 \n",
      " MAX LOSS: tensor(0.1376, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9995915032679739\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 18 -----------------------------------\n",
      " EPOCH 18 MINIBATCH: 305/306 LOSS: tensor(1.4156e-07, device='cuda:0') \n",
      " EPOCH 18 LOSS tensor(0.0003, device='cuda:0') ETA: 2.460038900375366 \n",
      " MAX LOSS: tensor(0.0459, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 19 -----------------------------------\n",
      " EPOCH 19 MINIBATCH: 305/306 LOSS: tensor(1.4901e-08, device='cuda:0') \n",
      " EPOCH 19 LOSS tensor(0.0005, device='cuda:0') ETA: 2.945845127105713 \n",
      " MAX LOSS: tensor(0.0552, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9997957516339869\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 20 -----------------------------------\n",
      " EPOCH 20 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0') uda:0')\n",
      " EPOCH 20 LOSS tensor(0.0007, device='cuda:0') ETA: 2.3904519081115723 \n",
      " MAX LOSS: tensor(0.0979, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9995915032679739\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 21 -----------------------------------\n",
      " EPOCH 21 MINIBATCH: 305/306 LOSS: tensor(3.7253e-08, device='cuda:0') \n",
      " EPOCH 21 LOSS tensor(0.0002, device='cuda:0') ETA: 2.3146231174468994 \n",
      " MAX LOSS: tensor(0.0299, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 22 -----------------------------------\n",
      " EPOCH 22 MINIBATCH: 305/306 LOSS: tensor(2.7827e-06, device='cuda:0') \n",
      " EPOCH 22 LOSS tensor(0.0002, device='cuda:0') ETA: 2.2610087394714355 \n",
      " MAX LOSS: tensor(0.0241, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 23 -----------------------------------\n",
      " EPOCH 23 MINIBATCH: 305/306 LOSS: tensor(3.8743e-07, device='cuda:0') \n",
      " EPOCH 23 LOSS tensor(0.0001, device='cuda:0') ETA: 2.315267324447632 \n",
      " MAX LOSS: tensor(0.0210, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 1.0\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 24 -----------------------------------\n",
      " EPOCH 24 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0') uda:0')\n",
      " EPOCH 24 LOSS tensor(0.0002, device='cuda:0') ETA: 2.7282748222351074 \n",
      " MAX LOSS: tensor(0.0391, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 25 -----------------------------------\n",
      " EPOCH 25 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0') uda:0')\n",
      " EPOCH 25 LOSS tensor(0.0002, device='cuda:0') ETA: 2.6581528186798096 \n",
      " MAX LOSS: tensor(0.0498, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 26 -----------------------------------\n",
      " EPOCH 26 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0') uda:0')\n",
      " EPOCH 26 LOSS tensor(0.0003, device='cuda:0') ETA: 2.4997262954711914 \n",
      " MAX LOSS: tensor(0.0360, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9997957516339869\n",
      "CORRECT: 190 INCORRECT: 2 TEST ACCURACY: 0.9895833333333334\n",
      "-----------------------EPOCH 27 -----------------------------------\n",
      " EPOCH 27 MINIBATCH: 305/306 LOSS: tensor(3.7253e-09, device='cuda:0') \n",
      " EPOCH 27 LOSS tensor(0.0008, device='cuda:0') ETA: 2.6667423248291016 \n",
      " MAX LOSS: tensor(0.0770, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9995915032679739\n",
      "CORRECT: 190 INCORRECT: 2 TEST ACCURACY: 0.9895833333333334\n",
      "-----------------------EPOCH 28 -----------------------------------\n",
      " EPOCH 28 MINIBATCH: 305/306 LOSS: tensor(7.4506e-09, device='cuda:0') \n",
      " EPOCH 28 LOSS tensor(0.0002, device='cuda:0') ETA: 2.42136812210083 \n",
      " MAX LOSS: tensor(0.0256, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 29 -----------------------------------\n",
      " EPOCH 29 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0')cuda:0') \n",
      " EPOCH 29 LOSS tensor(0.0005, device='cuda:0') ETA: 2.4126811027526855 \n",
      " MAX LOSS: tensor(0.0891, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9997957516339869\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 30 -----------------------------------\n",
      " EPOCH 30 MINIBATCH: 305/306 LOSS: tensor(5.5506e-07, device='cuda:0') \n",
      " EPOCH 30 LOSS tensor(8.3917e-05, device='cuda:0') ETA: 2.2079310417175293 \n",
      " MAX LOSS: tensor(0.0173, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 1.0\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 31 -----------------------------------\n",
      " EPOCH 31 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0') 0'):0')\n",
      " EPOCH 31 LOSS tensor(0.0006, device='cuda:0') ETA: 2.230055093765259 \n",
      " MAX LOSS: tensor(0.0748, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9997957516339869\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 32 -----------------------------------\n",
      " EPOCH 32 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0')cuda:0') \n",
      " EPOCH 32 LOSS tensor(0.0005, device='cuda:0') ETA: 2.246206045150757 \n",
      " MAX LOSS: tensor(0.1019, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9997957516339869\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 33 -----------------------------------\n",
      " EPOCH 33 MINIBATCH: 305/306 LOSS: tensor(0.0002, device='cuda:0'):0') \n",
      " EPOCH 33 LOSS tensor(0.0004, device='cuda:0') ETA: 2.5118019580841064 \n",
      " MAX LOSS: tensor(0.1072, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 34 -----------------------------------\n",
      " EPOCH 34 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0') uda:0')\n",
      " EPOCH 34 LOSS tensor(0.0003, device='cuda:0') ETA: 2.596909761428833 \n",
      " MAX LOSS: tensor(0.0530, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 190 INCORRECT: 2 TEST ACCURACY: 0.9895833333333334\n",
      "-----------------------EPOCH 35 -----------------------------------\n",
      " EPOCH 35 MINIBATCH: 305/306 LOSS: tensor(0.0009, device='cuda:0'):0') \n",
      " EPOCH 35 LOSS tensor(0.0003, device='cuda:0') ETA: 2.331319570541382 \n",
      " MAX LOSS: tensor(0.0461, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9997957516339869\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 36 -----------------------------------\n",
      " EPOCH 36 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0'):0'):0') \n",
      " EPOCH 36 LOSS tensor(0.0001, device='cuda:0') ETA: 2.3568031787872314 \n",
      " MAX LOSS: tensor(0.0172, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 1.0\n",
      "CORRECT: 191 INCORRECT: 1 TEST ACCURACY: 0.9947916666666666\n",
      "-----------------------EPOCH 37 -----------------------------------\n",
      " EPOCH 37 MINIBATCH: 305/306 LOSS: tensor(0., device='cuda:0') uda:0')\n",
      " EPOCH 37 LOSS tensor(0.0005, device='cuda:0') ETA: 2.2489378452301025 \n",
      " MAX LOSS: tensor(0.0926, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9997957516339869\n",
      "CORRECT: 190 INCORRECT: 2 TEST ACCURACY: 0.9895833333333334\n",
      "-----------------------EPOCH 38 -----------------------------------\n",
      " EPOCH 38 MINIBATCH: 305/306 LOSS: tensor(2.9802e-08, device='cuda:0') \n",
      " EPOCH 38 LOSS tensor(0.0002, device='cuda:0') ETA: 2.269012451171875 \n",
      " MAX LOSS: tensor(0.0543, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n",
      "-----------------------EPOCH 39 -----------------------------------\n",
      " EPOCH 39 MINIBATCH: 305/306 LOSS: tensor(7.4506e-09, device='cuda:0') \n",
      " EPOCH 39 LOSS tensor(0.0008, device='cuda:0') ETA: 2.2112414836883545 \n",
      " MAX LOSS: tensor(0.2145, device='cuda:0') MIN LOSS: tensor(0., device='cuda:0') TRAIN ACCURACY: 0.9998978758169934\n",
      "CORRECT: 192 INCORRECT: 0 TEST ACCURACY: 1.0\n"
     ]
    }
   ],
   "source": [
    "avg_test_loss_history = []\n",
    "avg_test_accuracy_history = []\n",
    "avg_train_loss_history = []\n",
    "avg_train_accuracy_history = []\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "new_best = 0\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "    start = time.time()\n",
    "    print(\n",
    "        \"-----------------------EPOCH \"\n",
    "        + str(i)\n",
    "        + \" -----------------------------------\"\n",
    "    )\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        if batch_idx == STEPS_PER_EPOCH:\n",
    "            break\n",
    "        optimizer.zero_grad()  # Resetting gradients after each optimizations\n",
    "        # Sending input , target to device\n",
    "        data = data.to(device) \n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.reshape((BATCH_SIZE,)).long())\n",
    "        loss_history.append(loss.detach())\n",
    "        # The loss variable has gradient attached to it so we are removing it so that it can be used to plot graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()  # Optimizing the model\n",
    "\n",
    "        # Checking train accuracy\n",
    "\n",
    "        correct = 0\n",
    "        incorrect = 0\n",
    "        for p in range(BATCH_SIZE):\n",
    "            a = []\n",
    "            for j in output[p]:\n",
    "                a.append(float(j.detach()))\n",
    "\n",
    "            pred = a.index(max(a))\n",
    "\n",
    "            if pred == int(target[p]):\n",
    "                correct = correct + 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                incorrect = incorrect + 1\n",
    "\n",
    "        print(\n",
    "            \"\\r EPOCH \"\n",
    "            + str(i)\n",
    "            + \" MINIBATCH: \"\n",
    "            + str(batch_idx)\n",
    "            + \"/\"\n",
    "            + str(STEPS_PER_EPOCH)\n",
    "            + \" LOSS: \"\n",
    "            + str(loss_history[-1]),\n",
    "            end = \"\"\n",
    "            \n",
    "        )\n",
    "        \n",
    "        accuracy_history.append(correct/(correct+incorrect))\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        \" \\n EPOCH \"\n",
    "        + str(i)\n",
    "        + \" LOSS \"\n",
    "        + str(sum(loss_history[-STEPS_PER_EPOCH:]) / STEPS_PER_EPOCH)\n",
    "        + \" ETA: \"\n",
    "        + str(end - start)\n",
    "        + \" \\n MAX LOSS: \"\n",
    "        + str(max(loss_history[-STEPS_PER_EPOCH:]))\n",
    "        + \" MIN LOSS: \"\n",
    "        + str(min(loss_history[-STEPS_PER_EPOCH:]))\n",
    "        + \" TRAIN ACCURACY: \"\n",
    "        + str(sum(accuracy_history[-STEPS_PER_EPOCH:]) / STEPS_PER_EPOCH)\n",
    "    )\n",
    "    \n",
    "    avg_train_loss_history.append(sum(loss_history[-STEPS_PER_EPOCH:]) / STEPS_PER_EPOCH)\n",
    "    avg_train_accuracy_history.append(sum(accuracy_history[-STEPS_PER_EPOCH:]) / STEPS_PER_EPOCH)\n",
    "\n",
    "    test_acc , test_loss  = get_test()\n",
    "    \n",
    "    avg_test_accuracy_history.append(test_acc)\n",
    "    avg_train_loss_history.append(test_loss)\n",
    "    \n",
    "    if test_acc>=new_best: \n",
    "        new_best = test_acc\n",
    "        torch.save(model.state_dict(), \"model_save/model.pth\") # Saving our best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc2a131-286e-4387-9421-504367a5c358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.1799446940422058\n",
      "Epoch 2/30, Loss: 0.19608958065509796\n",
      "Epoch 3/30, Loss: 0.18392331898212433\n",
      "Epoch 4/30, Loss: 0.23482763767242432\n",
      "Epoch 5/30, Loss: 0.13934367895126343\n",
      "Epoch 6/30, Loss: 0.18541790544986725\n",
      "Epoch 7/30, Loss: 0.15175534784793854\n",
      "Epoch 8/30, Loss: 0.17478160560131073\n",
      "Epoch 9/30, Loss: 0.19726961851119995\n",
      "Epoch 10/30, Loss: 0.15457020699977875\n",
      "Epoch 11/30, Loss: 0.13885794579982758\n",
      "Epoch 12/30, Loss: 0.1480586677789688\n",
      "Epoch 13/30, Loss: 0.1342870146036148\n",
      "Epoch 14/30, Loss: 0.14605091512203217\n",
      "Epoch 15/30, Loss: 0.13792386651039124\n",
      "Epoch 16/30, Loss: 0.13033649325370789\n",
      "Epoch 17/30, Loss: 0.15555238723754883\n",
      "Epoch 18/30, Loss: 0.17800016701221466\n",
      "Epoch 19/30, Loss: 0.18443433940410614\n",
      "Epoch 20/30, Loss: 0.19879984855651855\n",
      "Epoch 21/30, Loss: 0.09065332263708115\n",
      "Epoch 22/30, Loss: 0.20447546243667603\n",
      "Epoch 23/30, Loss: 0.15304066240787506\n",
      "Epoch 24/30, Loss: 0.19165651500225067\n",
      "Epoch 25/30, Loss: 0.1483944058418274\n",
      "Epoch 26/30, Loss: 0.12879560887813568\n",
      "Epoch 27/30, Loss: 0.16794578731060028\n",
      "Epoch 28/30, Loss: 0.1753036230802536\n",
      "Epoch 29/30, Loss: 0.1709127575159073\n",
      "Epoch 30/30, Loss: 0.15503470599651337\n",
      "Calculating reconstruction error threshold...\n",
      "Calculated threshold: 0.4672616183701406\n"
     ]
    }
   ],
   "source": [
    "# autoencoder, works only on clean data\n",
    "import torch.optim as optim\n",
    "\n",
    "# define an autoencoder, downsampling twice before upsampling back to original\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # output in range [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.0007)\n",
    "\n",
    "# training\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    for images, _ in trainloader:  # labels aren't used\n",
    "        outputs = autoencoder(images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Calculating reconstruction error threshold...\")\n",
    "reconstruction_errors = []\n",
    "for images, _ in trainloader:  # use clean training data\n",
    "    for image in images:\n",
    "        with torch.no_grad():\n",
    "            reconstructed = autoencoder(image.unsqueeze(0))\n",
    "            error = torch.mean((image - reconstructed.squeeze()) ** 2).item()\n",
    "            reconstruction_errors.append(error)\n",
    "\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_dev = np.std(reconstruction_errors)\n",
    "threshold = mean_error + 3 * std_dev  # 99% confidence\n",
    "print(f\"Calculated threshold: {threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "60e28fc0-05b7-4415-b485-8648e8ccc006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY :  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"ACCURACY : \",new_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c0ec37-9fc3-49ca-b5c6-e78eae39e459",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_train_accuracy_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mavg_train_accuracy_history\u001b[49m , label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(avg_test_accuracy_history , label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mACCURACY PER EPOCH\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avg_train_accuracy_history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(avg_train_accuracy_history , label = \"Train\")\n",
    "plt.plot(avg_test_accuracy_history , label = \"Test\")\n",
    "plt.title('ACCURACY PER EPOCH')\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"ACCURACY\")\n",
    "plt.legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da01cd3c-989f-4d86-a285-9cbf665152d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, img_path  \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "testset = CustomTestDataset(root_dir='data/allfrog/frog', transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b8f43df-b0be-4855-9e0c-e6ad7edfc1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in case of model mismatch error\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6b44f4f-6b65-44ca-857b-4147edae6f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to labels.txt\n"
     ]
    }
   ],
   "source": [
    "# now with the model trained, run the model against the primary poisoned data\n",
    "model.eval() \n",
    "\n",
    "predictions = []\n",
    "image_paths = []\n",
    "with torch.no_grad():\n",
    "    for images, paths in testloader:\n",
    "        images = images.to(device) # ensure images are on the device too\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the predicted class (0 for frog, 1 for truck)\n",
    "        \n",
    "        predictions.extend(predicted.tolist())\n",
    "        image_paths.extend(paths)\n",
    "\n",
    "# Map predictions to 'good' (0) and 'bad' (1), then sort them\n",
    "result_labels = ['good' if pred == 0 else 'bad' for pred in predictions]\n",
    "sorted_paths, sorted_labels = zip(*sorted(zip(image_paths, result_labels)))\n",
    "\n",
    "# Write sorted labels to the labels.txt file\n",
    "with open(\"labels.txt\", 'w') as f:\n",
    "    for label in sorted_labels:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to labels.txt\") # Confirmation that the predictions have finished\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84fcb415-c2e8-447f-a9d9-fa3e39597e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting anomalies in the allfrog dataset...\n",
      "Anomalies detection complete, results saved to label.txt\n"
     ]
    }
   ],
   "source": [
    "# evaluate allfrog with autoencoder\n",
    "model.eval()\n",
    "\n",
    "def detect_anomaly(image, threshold):\n",
    "    with torch.no_grad():\n",
    "        reconstructed = autoencoder(image.unsqueeze(0))\n",
    "        error = torch.mean((image - reconstructed.squeeze()) ** 2).item()\n",
    "        return error > threshold, error\n",
    "\n",
    "print(\"Detecting anomalies in the allfrog dataset...\")\n",
    "results = []\n",
    "\n",
    "for images, paths in testloader:  \n",
    "    for i, image in enumerate(images):\n",
    "        is_anomaly, _ = detect_anomaly(image, threshold)\n",
    "        label = \"bad\" if is_anomaly else \"good\" # good for frogs, bad for not frogs\n",
    "        results.append((paths[i], label))\n",
    "\n",
    "results.sort(key=lambda x: x[0]) # sorts by file names\n",
    "\n",
    "with open(\"labels.txt\", \"w\") as f:\n",
    "    for _, label in results:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Anomalies detection complete, results saved to label.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "077bcfbe-330d-4253-996f-ac9324c091ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spread of labels:\n",
      "Good: 5148\n",
      "Bad: 352\n",
      "Percentage of poisoned frogs identified: 70.40%\n",
      "\n",
      "Results appended to results.txt\n"
     ]
    }
   ],
   "source": [
    "# print out some result data for easy analysis\n",
    "with open(\"labels.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "labels = [line.strip() for line in lines]\n",
    "\n",
    "good_count = labels.count(\"good\")\n",
    "bad_count = labels.count(\"bad\")\n",
    "\n",
    "results = f\"Spread of labels:\\nGood: {good_count}\\nBad: {bad_count}\\n\"\n",
    "\n",
    "\n",
    "total_poisoned_frogs = 500\n",
    "correct_poisoned_frogs = bad_count  # assuming all predictions are correct\n",
    "\n",
    "correct_poisoned_frog_percentage = (correct_poisoned_frogs / total_poisoned_frogs) * 100\n",
    "\n",
    "results += f\"Percentage of poisoned frogs identified: {correct_poisoned_frog_percentage:.2f}%\\n\"\n",
    "print(results)\n",
    "\n",
    "with open(\"results.txt\", 'a') as r:\n",
    "    r.write(results)\n",
    "    r.write(\"\\n\" + \"=\"*40 + \"\\n\\n\") # dividing line for more clarity\n",
    "\n",
    "print(f\"Results appended to results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5203ebe-8e2e-48a2-a4a7-34f50d5cce51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
